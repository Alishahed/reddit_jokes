{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/alishahed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    31576\n",
      "1    31576\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1382/1382 [00:34<00:00, 40.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6765641542057225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1382/1382 [00:35<00:00, 39.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.5197297176063147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1382/1382 [00:35<00:00, 39.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.26738519113181985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1382/1382 [00:35<00:00, 38.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.1402523130355012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1382/1382 [00:36<00:00, 37.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.08333283449777883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1382/1382 [00:35<00:00, 38.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.059937963605044324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1382/1382 [00:35<00:00, 38.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.042266305977601275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1382/1382 [00:35<00:00, 38.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.033750013039159615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1382/1382 [00:35<00:00, 38.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.027996766835212905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1382/1382 [00:35<00:00, 38.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.020693779186223157\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.89      0.94      5715\n",
      "        True       0.90      0.98      0.94      5653\n",
      "\n",
      "    accuracy                           0.94     11368\n",
      "   macro avg       0.94      0.94      0.94     11368\n",
      "weighted avg       0.94      0.94      0.94     11368\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "pd.set_option('display.max_colwidth', 3000)\n",
    "pd.set_option('display.max_rows', 3000)\n",
    "pd.set_option('display.max_columns', 3000)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('./data/reddit_jokes_slim_processed.csv')\n",
    "\n",
    "# Text Preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Tokenize and lower case\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "df['text'] = df['thread_title'] + ' ' + df['thread_selftext']\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Build a vocabulary\n",
    "word_counts = Counter()\n",
    "for text in df['text']:\n",
    "    word_counts.update(text)\n",
    "vocab = {word: i + 1 for i, (word, _) in enumerate(word_counts.most_common())} # +1 for padding token\n",
    "\n",
    "# Encoding text and labels\n",
    "def encode_text(text, vocab):\n",
    "    return [vocab[word] for word in text if word in vocab]\n",
    "\n",
    "df['encoded_text'] = df['text'].apply(lambda x: encode_text(x, vocab))\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['thread_over_18'])\n",
    "\n",
    "# Padding sequences\n",
    "def pad_sequences(encoded_texts, max_length):\n",
    "    padded = np.zeros((len(encoded_texts), max_length), dtype=int)\n",
    "    for i, row in enumerate(encoded_texts):\n",
    "        padded[i, :len(row)] = np.array(row)[:max_length]\n",
    "    return padded\n",
    "\n",
    "max_length = 50 # You might want to set this to the length of your longest joke\n",
    "df['padded_text'] = list(pad_sequences(df['encoded_text'], max_length))\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df[df.label == 0]\n",
    "df_minority = df[df.label == 1]\n",
    "\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=len(df_majority),    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Display new class counts\n",
    "print(df_upsampled.label.value_counts())\n",
    "\n",
    "# Now use df_upsampled instead of df to split your dataset\n",
    "train, test = train_test_split(df_upsampled, test_size=0.3)\n",
    "val, test = train_test_split(test, test_size=0.6)\n",
    "\n",
    "# PyTorch dataset\n",
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.tensor(self.dataframe.iloc[idx]['padded_text'])\n",
    "        label = torch.tensor(self.dataframe.iloc[idx]['label'])\n",
    "        return text, label\n",
    "\n",
    "# LSTM Model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding_dim = embedding_dim  # Add this line\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(sentence.size(0), -1, self.embedding_dim))\n",
    "        lstm_out_last = lstm_out[:, -1, :]\n",
    "        label_space = self.hidden2label(lstm_out_last)\n",
    "        label_scores = nn.functional.log_softmax(label_space, dim=1)\n",
    "        return label_scores\n",
    "\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "vocab_size = len(vocab) + 1 # +1 for padding token\n",
    "model = LSTMClassifier(embedding_dim=100, hidden_dim=128, vocab_size=vocab_size, label_size=2)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and Evaluation Functions\n",
    "def train_model(model, train_loader, val_loader, optimizer, loss_function, epochs=1):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for texts, labels in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "        evaluate_model(model, val_loader)\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_loader:\n",
    "            outputs = model(texts)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "    return all_preds, all_labels\n",
    "\n",
    "\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(JokesDataset(train), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(JokesDataset(val), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(JokesDataset(test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, optimizer, loss_function, epochs=10)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_preds, test_labels = evaluate_model(model, test_loader)\n",
    "# Convert class labels to strings\n",
    "class_names = [str(cls) for cls in label_encoder.classes_]\n",
    "\n",
    "# Calculate and print precision, recall, and F1-score\n",
    "print(classification_report(test_labels, test_preds, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), 'models/jokes_adult_clean_classifier_lstm.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferernce with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke: Why don't scientists trust atoms? Because they make up everything!\n",
      "Classified as: Clean\n",
      "\n",
      "Joke: What do you call a fake noodle? An impasta!\n",
      "Classified as: Clean\n",
      "\n",
      "Joke: Why does Dr. Pepper come in a bottle? Because his wife died.\n",
      "Classified as: Clean\n",
      "\n",
      "Joke: What's an astronaut's favorite part of the computer? The space bar.\n",
      "Classified as: Clean\n",
      "\n",
      "Joke: Why did the bicycle fall over? It was two-tired.\n",
      "Classified as: Clean\n",
      "\n",
      "Joke: What do you call cheese that isn't yours? Nacho cheese.\n",
      "Classified as: Clean\n",
      "\n",
      "Joke: A guy is sitting at the doctor's office. The doctor walks in and says, 'I have some bad news. I'm afraid you're going to have to stop masturbating.' 'I don't understand, doc,' the patient says. 'Why?' 'Because,' the doctor says. 'I'm trying to examine you.'\n",
      "Classified as: Adult\n",
      "\n",
      "Joke: Why couldn't the bicycle stand up by itself? It was two-tired.\n",
      "Classified as: Clean\n",
      "\n",
      "Joke: Dear NASA: Your momma thought I was big enough. From, Pluto\n",
      "Classified as: Clean\n",
      "\n",
      "Joke: Why don’t skeletons fight each other? They don’t have the guts.\n",
      "Classified as: Clean\n",
      "\n",
      "Joke: What did the grape do when he got stepped on? He let out a little wine.\n",
      "Classified as: Clean\n",
      "\n",
      "Joke: What does the receptionist at a sperm bank say as clients leave? 'Thanks for coming!'\n",
      "Classified as: Clean\n",
      "\n",
      "Joke: Why don't some couples go to the gym? Because some relationships don't work out!\n",
      "Classified as: Clean\n",
      "\n",
      "Joke: What goes in hard and dry, but comes out soft and wet? Gum.\n",
      "Classified as: Adult\n",
      "\n",
      "Joke: I told my suitcases there will be no vacation this year. Now I'm dealing with emotional baggage.\n",
      "Classified as: Clean\n",
      "\n",
      "Joke: It's inappropriate to make a 'dad joke' if you're not a dad. It's a faux pa.\n",
      "Classified as: Clean\n",
      "\n",
      "Joke: I used to play piano by ear, but now I use my hands.\n",
      "Classified as: Clean\n",
      "\n",
      "Joke: What did the toaster say to the slice of bread? 'I want you inside me.'\n",
      "Classified as: Clean\n",
      "\n",
      "Joke: 'Give it to me! Give it to me!' she yelled. 'I'm so wet, give it to me now!' She could scream all she wanted, but I was keeping the umbrella.\n",
      "Classified as: Adult\n",
      "\n",
      "Joke: How do you embarrass an archaeologist? Give him a used tampon and ask him which period it came from.\n",
      "Classified as: Adult\n",
      "\n",
      "Joke: My wife told me to take the spider out instead of killing it. We went and had some drinks. Cool guy, wants to be a web developer.\n",
      "Classified as: Clean\n",
      "\n",
      "Joke: What do you call a smiling Roman soldier with a piece of hair stuck between his front teeth? A glad-he-ate-her.\n",
      "Classified as: Clean\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_new_jokes(jokes, vocab, max_length):\n",
    "    # Tokenize, encode, and pad the new jokes\n",
    "    tokenized_jokes = [word_tokenize(joke.lower()) for joke in jokes]\n",
    "    encoded_jokes = [[vocab.get(word, 0) for word in joke] for joke in tokenized_jokes]  # 0 for unknown words\n",
    "    padded_jokes = pad_sequences(encoded_jokes, max_length)\n",
    "    return padded_jokes\n",
    "\n",
    "def predict_jokes(model, jokes_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(jokes_tensor)\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "        return predictions\n",
    "\n",
    "# Example new jokes\n",
    "new_jokes = [\"Why don't scientists trust atoms? Because they make up everything!\",\n",
    "\"What do you call a fake noodle? An impasta!\",\n",
    "\"Why does Dr. Pepper come in a bottle? Because his wife died.\",\n",
    "\"What's an astronaut's favorite part of the computer? The space bar.\",\n",
    "\"Why did the bicycle fall over? It was two-tired.\",\n",
    "\"What do you call cheese that isn't yours? Nacho cheese.\",\n",
    "\"A guy is sitting at the doctor's office. The doctor walks in and says, 'I have some bad news. I'm afraid you're going to have to stop masturbating.' 'I don't understand, doc,' the patient says. 'Why?' 'Because,' the doctor says. 'I'm trying to examine you.'\",\n",
    "\"Why couldn't the bicycle stand up by itself? It was two-tired.\",\n",
    "\"Dear NASA: Your momma thought I was big enough. From, Pluto\",\n",
    "\"Why don’t skeletons fight each other? They don’t have the guts.\",\n",
    "\"What did the grape do when he got stepped on? He let out a little wine.\",\n",
    "\"What does the receptionist at a sperm bank say as clients leave? 'Thanks for coming!'\",\n",
    "\"Why don't some couples go to the gym? Because some relationships don't work out!\",\n",
    "\"What goes in hard and dry, but comes out soft and wet? Gum.\",\n",
    "\"I told my suitcases there will be no vacation this year. Now I'm dealing with emotional baggage.\",\n",
    "\"It's inappropriate to make a 'dad joke' if you're not a dad. It's a faux pa.\",\n",
    "\"I used to play piano by ear, but now I use my hands.\",\n",
    "\"What did the toaster say to the slice of bread? 'I want you inside me.'\",\n",
    "\"'Give it to me! Give it to me!' she yelled. 'I'm so wet, give it to me now!' She could scream all she wanted, but I was keeping the umbrella.\",\n",
    "\"How do you embarrass an archaeologist? Give him a used tampon and ask him which period it came from.\",\n",
    "\"My wife told me to take the spider out instead of killing it. We went and had some drinks. Cool guy, wants to be a web developer.\",\n",
    "\"What do you call a smiling Roman soldier with a piece of hair stuck between his front teeth? A glad-he-ate-her.\"]\n",
    "\n",
    "# Preprocess the jokes\n",
    "preprocessed_jokes = preprocess_new_jokes(new_jokes, vocab, max_length)\n",
    "jokes_tensor = torch.tensor(preprocessed_jokes)\n",
    "\n",
    "# Get predictions\n",
    "model = LSTMClassifier(embedding_dim=100, hidden_dim=128, vocab_size=vocab_size, label_size=2)\n",
    "\n",
    "# Load the model's state dictionary\n",
    "model.load_state_dict(torch.load('models/jokes_adult_clean_classifier_lstm.pth'))\n",
    "\n",
    "predictions = predict_jokes(model, jokes_tensor)\n",
    "\n",
    "# Convert predictions to labels\n",
    "predicted_labels = [label_encoder.classes_[pred] for pred in predictions]\n",
    "\n",
    "# Print results\n",
    "for joke, label in zip(new_jokes, predicted_labels):\n",
    "    print(f\"Joke: {joke}\\nClassified as: {'Adult' if label else 'Clean'}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_ve_39_DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
